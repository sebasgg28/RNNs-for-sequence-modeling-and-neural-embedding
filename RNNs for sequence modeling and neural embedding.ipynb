{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\"> RNNs for sequence modeling and neural embedding</span>\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Set random seeds</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(6789)\n",
    "np.random.seed(6789)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 1: Download and preprocess the data</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we use for this assignment is a question classification dataset for which the train set consists of $5,500$ questions belonging to 6 coarse question categories including:\n",
    "- abbreviation (ABBR), \n",
    "- entity (ENTY), \n",
    "- description (DESC), \n",
    "- human (HUM), \n",
    "- location (LOC) and \n",
    "- numeric (NUM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing data is an inital and important step in any machine learning or deep learning projects. The following *DataManager* class helps you to download data and preprocess data for the later steps of a deep learning project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import collections\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "class DataManager:\n",
    "    def __init__(self, verbose=True, maxlen= 50, random_state=6789):\n",
    "        self.verbose = verbose\n",
    "        self.max_sentence_len = 0\n",
    "        self.str_questions = list()\n",
    "        self.str_labels = list()\n",
    "        self.numeral_labels = list()\n",
    "        self.maxlen = maxlen\n",
    "        self.numeral_data = list()\n",
    "        self.random_state = random_state\n",
    "        self.random = np.random.RandomState(random_state)\n",
    "        \n",
    "    @staticmethod\n",
    "    def maybe_download(dir_name, file_name, url, verbose= True):\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.mkdir(dir_name)\n",
    "        if not os.path.exists(os.path.join(dir_name, file_name)):\n",
    "            urlretrieve(url + file_name, os.path.join(dir_name, file_name))\n",
    "        if verbose:\n",
    "            print(\"Downloaded successfully {}\".format(file_name))\n",
    "    \n",
    "    def read_data(self, dir_name, file_names):\n",
    "        for file_name in file_names:\n",
    "            file_path= os.path.join(dir_name, file_name)\n",
    "            self.str_questions= list(); self.str_labels= list()\n",
    "            with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
    "                for row in f:\n",
    "                    row_str= row.split(\":\")\n",
    "                    label, question= row_str[0], row_str[1]\n",
    "                    question= question.lower()\n",
    "                    self.str_labels.append(label)\n",
    "                    self.str_questions.append(question[0:-1])\n",
    "                    if self.max_sentence_len < len(self.str_questions[-1]):\n",
    "                        self.max_sentence_len= len(self.str_questions[-1])\n",
    "         \n",
    "        # turns labels into numbers\n",
    "        le= preprocessing.LabelEncoder()\n",
    "        le.fit(self.str_labels)\n",
    "        self.numeral_labels = np.array(le.transform(self.str_labels))\n",
    "        self.str_classes= le.classes_\n",
    "        self.num_classes= len(self.str_classes)\n",
    "        if self.verbose:\n",
    "            print(\"\\nSample questions... \\n\")\n",
    "            print(self.str_questions[0:5])\n",
    "            print(\"Labels {}\\n\\n\".format(self.str_classes))\n",
    "    \n",
    "    def manipulate_data(self):\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "        tokenizer.fit_on_texts(self.str_questions)\n",
    "        self.numeral_data = tokenizer.texts_to_sequences(self.str_questions)\n",
    "        self.numeral_data = tf.keras.preprocessing.sequence.pad_sequences(self.numeral_data, padding='post', truncating= 'post', maxlen= self.maxlen)\n",
    "        self.word2idx = tokenizer.word_index\n",
    "        self.word2idx = {k:v for k,v in self.word2idx.items()}\n",
    "        self.idx2word = {v:k for k,v in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "    \n",
    "    def train_valid_split(self, train_ratio=0.9):\n",
    "        idxs = np.random.permutation(np.arange(len(self.str_questions)))\n",
    "        train_size = int(train_ratio*len(idxs)) +1\n",
    "        self.train_str_questions, self.valid_str_questions = self.str_questions[0:train_size], self.str_questions[train_size:]\n",
    "        self.train_numeral_data, self.valid_numeral_data = self.numeral_data[0:train_size], self.numeral_data[train_size:]\n",
    "        self.train_numeral_labels, self.valid_numeral_labels = self.numeral_labels[0:train_size], self.numeral_labels[train_size:]\n",
    "        self.tf_train_set = tf.data.Dataset.from_tensor_slices((self.train_numeral_data, self.train_numeral_labels))\n",
    "        self.tf_valid_set = tf.data.Dataset.from_tensor_slices((self.valid_numeral_data, self.valid_numeral_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Sample questions... \n",
      "\n",
      "['manner how did serfdom develop in and then leave russia ?', 'cremat what films featured the character popeye doyle ?', \"manner how can i find a list of celebrities ' real names ?\", 'animal what fowl grabs the spotlight after the chinese year of the monkey ?', 'exp what is the full form of .com ?']\n",
      "Labels ['ABBR' 'DESC' 'ENTY' 'HUM' 'LOC' 'NUM']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "\n",
    "dm = DataManager(maxlen=100)\n",
    "dm.read_data(\"Data/\", [\"train_set.label\"])   # read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.manipulate_data()\n",
    "dm.train_valid_split(train_ratio=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a data manager, named *dm* containing the training and validiation sets in both text and numeric forms. Your task is to play around and read this code to figure out the meanings of some important attributes that will be used in the next parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self.tf_train_set will contain all of the train data set as tensors, it will make sure that all of the objects have the same dimesion. It will also combine the vectors of the labels and questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for valid_numeral_data, valid_numeral_labels in dm.tf_train_set:\n",
    "      print(valid_numeral_data, valid_numeral_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/5tensors.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self.tf_tvalid_set will contain all of the validation data set as tensors, it will make sure that all of the objects have the same dimesion. It will also combine the vectors of the labels and questions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for valid_numeral_data, valid_numeral_labels in dm.tf_valid_set:\n",
    "       print(valid_numeral_data, valid_numeral_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/5tensors2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Building the RNN </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " One-directional RNNs for sequence modeling and neural embedding </span> \n",
    "\n",
    "We'll test different RNN attributes in order to obtain the most accurate model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**In this part, we'll construct an RNN to learn from the dataset of interest. Building a basic RNN with the following requirements:**\n",
    "- Attribute `data_manager (self.data_manager)`: specifies the data manager used to store data for the model.\n",
    "- Attribute `cell_type (self.cell_type)`: can receive three values including `basic_rnn`, `gru`, and `lstm` which specifies the memory cells formed a hidden layer.\n",
    "- `state_sizes (self.state_sizes)` indicates the list of the hidden sizes from the second hidden layers of memory cells. For example, $embed\\_size =128$ and $state\\_sizes = [64, 64]$ means that you have three hidden layers in your network with hidden sizes of $128, 64$ and $64$ respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniRNN:\n",
    "    def __init__(self, cell_type= 'gru', embed_size= 128, state_sizes= [128, 64], data_manager= None):\n",
    "        self.cell_type = cell_type\n",
    "        self.state_sizes = state_sizes\n",
    "        self.embed_size = embed_size\n",
    "        self.data_manager = data_manager\n",
    "        self.vocab_size = self.data_manager.vocab_size +1 \n",
    "        \n",
    "    #return the correspoding memory cell\n",
    "    @staticmethod\n",
    "    def get_layer(cell_type= 'gru', state_size= 128, return_sequences= False, activation = 'tanh'):\n",
    "        if cell_type=='gru':\n",
    "            return  tf.keras.layers.GRU(state_size, return_sequences=return_sequences,activation=activation)\n",
    "        elif cell_type== 'lstm':\n",
    "            return tf.keras.layers.LSTM(state_size, return_sequences=return_sequences,activation=activation)\n",
    "        else:\n",
    "            return tf.keras.layers.SimpleRNN(state_size, return_sequences=return_sequences,activation=activation)\n",
    "    \n",
    "    def build(self):\n",
    "        x = tf.keras.layers.Input(shape=[None])\n",
    "        h = tf.keras.layers.Embedding(self.vocab_size, self.embed_size,mask_zero=True, trainable= True)(x)\n",
    "        num_layers = len(self.state_sizes)\n",
    "        for i in range(num_layers):\n",
    "            h = self.get_layer(self.cell_type, self.state_sizes[i], return_sequences=True)(h)\n",
    "        h = self.get_layer(self.cell_type, self.state_sizes[i], return_sequences=False)(h)\n",
    "        h = tf.keras.layers.Dense(dm.num_classes, activation='softmax')(h)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h)\n",
    "   \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Run with basic RNN ('basic_rnn') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps, validate for 35 steps\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - 41s 783ms/step - loss: 0.5538 - accuracy: 0.8072 - val_loss: 0.2692 - val_accuracy: 0.9239\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - 37s 720ms/step - loss: 0.2037 - accuracy: 0.9355 - val_loss: 0.1255 - val_accuracy: 0.9656\n",
      "Epoch 3/20\n",
      "52/52 [==============================] - 38s 729ms/step - loss: 0.0982 - accuracy: 0.9707 - val_loss: 0.0950 - val_accuracy: 0.9633\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - 38s 725ms/step - loss: 0.1072 - accuracy: 0.9694 - val_loss: 0.1130 - val_accuracy: 0.9619\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - 38s 728ms/step - loss: 0.0416 - accuracy: 0.9884 - val_loss: 0.1202 - val_accuracy: 0.9647\n",
      "Epoch 6/20\n",
      "52/52 [==============================] - 38s 734ms/step - loss: 0.0484 - accuracy: 0.9853 - val_loss: 0.2755 - val_accuracy: 0.9284\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - 38s 724ms/step - loss: 0.0670 - accuracy: 0.9811 - val_loss: 0.1028 - val_accuracy: 0.9743\n",
      "Epoch 8/20\n",
      "52/52 [==============================] - 38s 733ms/step - loss: 0.0520 - accuracy: 0.9902 - val_loss: 0.1394 - val_accuracy: 0.9624\n",
      "Epoch 9/20\n",
      "52/52 [==============================] - 38s 728ms/step - loss: 0.0136 - accuracy: 0.9966 - val_loss: 0.1216 - val_accuracy: 0.9693\n",
      "Epoch 10/20\n",
      "52/52 [==============================] - 38s 725ms/step - loss: 0.0443 - accuracy: 0.9893 - val_loss: 0.1573 - val_accuracy: 0.9679\n",
      "Epoch 11/20\n",
      "52/52 [==============================] - 38s 727ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1481 - val_accuracy: 0.9702\n",
      "Epoch 12/20\n",
      "52/52 [==============================] - 38s 728ms/step - loss: 2.1410e-04 - accuracy: 1.0000 - val_loss: 0.1905 - val_accuracy: 0.9683\n",
      "Epoch 13/20\n",
      "52/52 [==============================] - 38s 732ms/step - loss: 0.0379 - accuracy: 0.9924 - val_loss: 0.1374 - val_accuracy: 0.9711\n",
      "Epoch 14/20\n",
      "52/52 [==============================] - 38s 736ms/step - loss: 0.0256 - accuracy: 0.9939 - val_loss: 0.1293 - val_accuracy: 0.9619\n",
      "Epoch 15/20\n",
      "52/52 [==============================] - 38s 730ms/step - loss: 0.0209 - accuracy: 0.9939 - val_loss: 0.1618 - val_accuracy: 0.9628\n",
      "Epoch 16/20\n",
      "52/52 [==============================] - 38s 731ms/step - loss: 0.0093 - accuracy: 0.9976 - val_loss: 0.1327 - val_accuracy: 0.9716\n",
      "Epoch 17/20\n",
      "52/52 [==============================] - 38s 734ms/step - loss: 7.7490e-05 - accuracy: 1.0000 - val_loss: 0.1471 - val_accuracy: 0.9702\n",
      "Epoch 18/20\n",
      "52/52 [==============================] - 38s 726ms/step - loss: 1.6607e-05 - accuracy: 1.0000 - val_loss: 0.1726 - val_accuracy: 0.9720\n",
      "Epoch 19/20\n",
      "52/52 [==============================] - 38s 734ms/step - loss: 0.0413 - accuracy: 0.9921 - val_loss: 0.1958 - val_accuracy: 0.9706\n",
      "Epoch 20/20\n",
      "52/52 [==============================] - 38s 729ms/step - loss: 0.0194 - accuracy: 0.9969 - val_loss: 0.1630 - val_accuracy: 0.9647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d062ee7188>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_rnn = UniRNN(cell_type=\"basic_rnn\",embed_size=128,state_sizes=[128,128],data_manager=dm)\n",
    "uni_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "uni_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "uni_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**GRU ('gru') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps, validate for 35 steps\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - 108s 2s/step - loss: 0.7327 - accuracy: 0.7161 - val_loss: 0.1815 - val_accuracy: 0.9385\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - 100s 2s/step - loss: 0.1271 - accuracy: 0.9569 - val_loss: 0.1124 - val_accuracy: 0.9651\n",
      "Epoch 3/20\n",
      "52/52 [==============================] - 100s 2s/step - loss: 0.0591 - accuracy: 0.9804 - val_loss: 0.1212 - val_accuracy: 0.9683\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - 102s 2s/step - loss: 0.0741 - accuracy: 0.9859 - val_loss: 0.2869 - val_accuracy: 0.9239\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - 103s 2s/step - loss: 0.0160 - accuracy: 0.9957 - val_loss: 0.6139 - val_accuracy: 0.9023\n",
      "Epoch 6/20\n",
      "52/52 [==============================] - 103s 2s/step - loss: 0.0362 - accuracy: 0.9924 - val_loss: 0.1505 - val_accuracy: 0.9679\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - 103s 2s/step - loss: 0.0087 - accuracy: 0.9966 - val_loss: 0.1350 - val_accuracy: 0.9670\n",
      "Epoch 8/20\n",
      "52/52 [==============================] - 105s 2s/step - loss: 0.0097 - accuracy: 0.9979 - val_loss: 0.1658 - val_accuracy: 0.9628\n",
      "Epoch 9/20\n",
      "52/52 [==============================] - 108s 2s/step - loss: 0.0066 - accuracy: 0.9994 - val_loss: 0.1856 - val_accuracy: 0.9665\n",
      "Epoch 10/20\n",
      "52/52 [==============================] - 109s 2s/step - loss: 0.0081 - accuracy: 0.9982 - val_loss: 0.2282 - val_accuracy: 0.9633\n",
      "Epoch 11/20\n",
      "52/52 [==============================] - 109s 2s/step - loss: 6.9877e-05 - accuracy: 1.0000 - val_loss: 0.2128 - val_accuracy: 0.9683\n",
      "Epoch 12/20\n",
      "52/52 [==============================] - 108s 2s/step - loss: 1.4597e-05 - accuracy: 1.0000 - val_loss: 0.2249 - val_accuracy: 0.9702\n",
      "Epoch 13/20\n",
      "52/52 [==============================] - 104s 2s/step - loss: 3.1935e-06 - accuracy: 1.0000 - val_loss: 0.2525 - val_accuracy: 0.9706\n",
      "Epoch 14/20\n",
      "52/52 [==============================] - 104s 2s/step - loss: 3.8809e-06 - accuracy: 1.0000 - val_loss: 0.2934 - val_accuracy: 0.9665\n",
      "Epoch 15/20\n",
      "52/52 [==============================] - 105s 2s/step - loss: 2.3263e-07 - accuracy: 1.0000 - val_loss: 0.3027 - val_accuracy: 0.9688\n",
      "Epoch 16/20\n",
      "52/52 [==============================] - 104s 2s/step - loss: 9.8415e-08 - accuracy: 1.0000 - val_loss: 0.3121 - val_accuracy: 0.9702\n",
      "Epoch 17/20\n",
      "52/52 [==============================] - 104s 2s/step - loss: 4.4068e-08 - accuracy: 1.0000 - val_loss: 0.3195 - val_accuracy: 0.9697\n",
      "Epoch 18/20\n",
      "52/52 [==============================] - 104s 2s/step - loss: 2.7698e-08 - accuracy: 1.0000 - val_loss: 0.3239 - val_accuracy: 0.9697\n",
      "Epoch 19/20\n",
      "52/52 [==============================] - 115s 2s/step - loss: 2.0675e-08 - accuracy: 1.0000 - val_loss: 0.3265 - val_accuracy: 0.9693\n",
      "Epoch 20/20\n",
      "52/52 [==============================] - 126s 2s/step - loss: 1.6232e-08 - accuracy: 1.0000 - val_loss: 0.3314 - val_accuracy: 0.9693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d08a906288>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_rnn = UniRNN(cell_type=\"gru\",embed_size=128,state_sizes=[128,128],data_manager=dm)\n",
    "uni_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "uni_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "uni_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**LSTM ('lstm') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps, validate for 35 steps\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - 128s 2s/step - loss: 0.7223 - accuracy: 0.7347 - val_loss: 0.2390 - val_accuracy: 0.9349\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - 120s 2s/step - loss: 0.1559 - accuracy: 0.9487 - val_loss: 0.1216 - val_accuracy: 0.9638\n",
      "Epoch 3/20\n",
      "52/52 [==============================] - 123s 2s/step - loss: 0.0884 - accuracy: 0.9716 - val_loss: 0.6112 - val_accuracy: 0.8830\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - 130s 3s/step - loss: 0.0846 - accuracy: 0.9762 - val_loss: 0.1161 - val_accuracy: 0.9670\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - 130s 2s/step - loss: 0.1014 - accuracy: 0.9765 - val_loss: 0.1355 - val_accuracy: 0.9628\n",
      "Epoch 6/20\n",
      "52/52 [==============================] - 131s 3s/step - loss: 0.0894 - accuracy: 0.9789 - val_loss: 0.1272 - val_accuracy: 0.9665\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - 129s 2s/step - loss: 0.0385 - accuracy: 0.9887 - val_loss: 0.1463 - val_accuracy: 0.9651\n",
      "Epoch 8/20\n",
      "52/52 [==============================] - 143s 3s/step - loss: 0.0268 - accuracy: 0.9902 - val_loss: 0.1644 - val_accuracy: 0.9596\n",
      "Epoch 9/20\n",
      "52/52 [==============================] - 146s 3s/step - loss: 0.0391 - accuracy: 0.9924 - val_loss: 0.2492 - val_accuracy: 0.9532\n",
      "Epoch 10/20\n",
      "52/52 [==============================] - 125s 2s/step - loss: 0.0430 - accuracy: 0.9914 - val_loss: 0.1768 - val_accuracy: 0.9642\n",
      "Epoch 11/20\n",
      "52/52 [==============================] - 132s 3s/step - loss: 0.0118 - accuracy: 0.9963 - val_loss: 0.2290 - val_accuracy: 0.9606\n",
      "Epoch 12/20\n",
      "52/52 [==============================] - 127s 2s/step - loss: 0.0129 - accuracy: 0.9972 - val_loss: 0.2490 - val_accuracy: 0.9587\n",
      "Epoch 13/20\n",
      "52/52 [==============================] - 124s 2s/step - loss: 0.0820 - accuracy: 0.9887 - val_loss: 0.2445 - val_accuracy: 0.9555\n",
      "Epoch 14/20\n",
      "52/52 [==============================] - 121s 2s/step - loss: 0.0052 - accuracy: 0.9991 - val_loss: 0.2522 - val_accuracy: 0.9583\n",
      "Epoch 15/20\n",
      "52/52 [==============================] - 121s 2s/step - loss: 0.0094 - accuracy: 0.9972 - val_loss: 0.2476 - val_accuracy: 0.9560\n",
      "Epoch 16/20\n",
      "52/52 [==============================] - 124s 2s/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.2812 - val_accuracy: 0.9578\n",
      "Epoch 17/20\n",
      "52/52 [==============================] - 130s 2s/step - loss: 0.0577 - accuracy: 0.9924 - val_loss: 0.3198 - val_accuracy: 0.9578\n",
      "Epoch 18/20\n",
      "52/52 [==============================] - 126s 2s/step - loss: 0.0046 - accuracy: 0.9994 - val_loss: 0.3160 - val_accuracy: 0.9596\n",
      "Epoch 19/20\n",
      "52/52 [==============================] - 124s 2s/step - loss: 0.0159 - accuracy: 0.9979 - val_loss: 0.3283 - val_accuracy: 0.9541\n",
      "Epoch 20/20\n",
      "52/52 [==============================] - 125s 2s/step - loss: 0.0075 - accuracy: 0.9988 - val_loss: 0.3210 - val_accuracy: 0.9514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d0a2dd7a48>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_rnn = UniRNN(cell_type=\"lstm\",embed_size=128,state_sizes=[128,128],data_manager=dm)\n",
    "uni_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "uni_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "uni_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From training our model we got excelent results, the lowest accuracy is well above 95%.The accuracy gotten from the model is with the GRU with 96.93%. It is roughly 0.5% more accurate than our simple Rnn which give us great results but having such high accuracy any percetage increase is a huge gain.\n",
    "\n",
    "As GRU vs LSTM, there is no saying on which one is better as it is adviced to try both to see which one yields more accurate restults. However, GRU is a clear winner on this one. Lets see how they perfom on the next model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-directional RNNs for sequence modeling and neural embedding </span> ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Building a Bi=directional RNN*.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN:\n",
    "    def __init__(self, cell_type= 'gru', embed_size= 128, state_sizes= [128, 64], data_manager= None):\n",
    "        self.cell_type = cell_type\n",
    "        self.state_sizes = state_sizes\n",
    "        self.embed_size = embed_size\n",
    "        self.data_manager = data_manager\n",
    "        self.vocab_size = self.data_manager.vocab_size +1\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_layer(cell_type= 'gru', state_size= 128, return_sequences= False, activation = 'tanh'):\n",
    "        if cell_type=='gru':\n",
    "            return  tf.keras.layers.Bidirectional(tf.keras.layers.GRU(state_size, return_sequences=return_sequences,activation=activation))\n",
    "        elif cell_type== 'lstm':\n",
    "            return  tf.keras.layers.Bidirectional((tf.keras.layers.LSTM(state_size, return_sequences=return_sequences,activation=activation)))\n",
    "        else:\n",
    "            return tf.keras.layers.Bidirectional(state_size, return_sequences=return_sequences,activation=activation)\n",
    "    \n",
    "    def build(self):\n",
    "        x = tf.keras.layers.Input(shape=[None])\n",
    "        h = tf.keras.layers.Embedding(self.vocab_size, self.embed_size,mask_zero=True, trainable= True)(x)\n",
    "        num_layers = len(self.state_sizes)\n",
    "        for i in range(num_layers):\n",
    "            h = self.get_layer(self.cell_type, self.state_sizes[i], return_sequences=True)(h)\n",
    "        h = self.get_layer(self.cell_type, self.state_sizes[i], return_sequences=False)(h)\n",
    "        h = tf.keras.layers.Dense(dm.num_classes, activation='softmax')(h)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h)\n",
    "        \n",
    "    \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**BiRNN for basic RNN ('basic_rnn') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_rnn =  BiRNN(cell_type=\"basic_rnn\",embed_size=128,state_sizes=[128,128],data_manager=dm)\n",
    "bi_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "bi_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "bi_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/1.BiRNN-Simple.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Running BiRNN for GRU ('gru') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps, validate for 35 steps\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - 428s 8s/step - loss: 0.5346 - accuracy: 0.8023 - val_loss: 0.1308 - val_accuracy: 0.9523\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - 418s 8s/step - loss: 0.0653 - accuracy: 0.9804 - val_loss: 0.0986 - val_accuracy: 0.9688\n",
      "Epoch 3/20\n",
      "52/52 [==============================] - 443s 9s/step - loss: 0.0368 - accuracy: 0.9908 - val_loss: 0.1073 - val_accuracy: 0.9720\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - 419s 8s/step - loss: 0.0057 - accuracy: 0.9988 - val_loss: 0.1441 - val_accuracy: 0.9670\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - 427s 8s/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.1810 - val_accuracy: 0.9647\n",
      "Epoch 6/20\n",
      "52/52 [==============================] - 436s 8s/step - loss: 3.8054e-04 - accuracy: 0.9997 - val_loss: 0.1546 - val_accuracy: 0.9706\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - 445s 9s/step - loss: 0.0123 - accuracy: 0.9982 - val_loss: 0.1070 - val_accuracy: 0.9739\n",
      "Epoch 8/20\n",
      "52/52 [==============================] - 445s 9s/step - loss: 6.0049e-04 - accuracy: 0.9997 - val_loss: 0.1317 - val_accuracy: 0.9766\n",
      "Epoch 9/20\n",
      "52/52 [==============================] - 460s 9s/step - loss: 2.4356e-04 - accuracy: 0.9997 - val_loss: 0.1710 - val_accuracy: 0.9757\n",
      "Epoch 10/20\n",
      "52/52 [==============================] - 426s 8s/step - loss: 4.0454e-06 - accuracy: 1.0000 - val_loss: 0.1613 - val_accuracy: 0.9775\n",
      "Epoch 11/20\n",
      "52/52 [==============================] - 424s 8s/step - loss: 4.9028e-07 - accuracy: 1.0000 - val_loss: 0.1631 - val_accuracy: 0.9775\n",
      "Epoch 12/20\n",
      "52/52 [==============================] - 423s 8s/step - loss: 1.3413e-07 - accuracy: 1.0000 - val_loss: 0.1734 - val_accuracy: 0.9743\n",
      "Epoch 13/20\n",
      "52/52 [==============================] - 422s 8s/step - loss: 5.2202e-08 - accuracy: 1.0000 - val_loss: 0.1823 - val_accuracy: 0.9771\n",
      "Epoch 14/20\n",
      "52/52 [==============================] - 879s 17s/step - loss: 2.6905e-08 - accuracy: 1.0000 - val_loss: 0.1808 - val_accuracy: 0.9761\n",
      "Epoch 15/20\n",
      "52/52 [==============================] - 424s 8s/step - loss: 1.4937e-08 - accuracy: 1.0000 - val_loss: 0.1821 - val_accuracy: 0.9771\n",
      "Epoch 16/20\n",
      "52/52 [==============================] - 425s 8s/step - loss: 1.0606e-08 - accuracy: 1.0000 - val_loss: 0.1840 - val_accuracy: 0.9771\n",
      "Epoch 17/20\n",
      "52/52 [==============================] - 425s 8s/step - loss: 8.6713e-09 - accuracy: 1.0000 - val_loss: 0.1854 - val_accuracy: 0.9771\n",
      "Epoch 18/20\n",
      "52/52 [==============================] - 884s 17s/step - loss: 6.8080e-09 - accuracy: 1.0000 - val_loss: 0.1864 - val_accuracy: 0.9771\n",
      "Epoch 19/20\n",
      "52/52 [==============================] - 452s 9s/step - loss: 5.6972e-09 - accuracy: 1.0000 - val_loss: 0.1876 - val_accuracy: 0.9771\n",
      "Epoch 20/20\n",
      "52/52 [==============================] - 444s 9s/step - loss: 4.9089e-09 - accuracy: 1.0000 - val_loss: 0.1883 - val_accuracy: 0.9775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d0dfb58b08>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_rnn = BiRNN(cell_type=\"gru\",embed_size=128,state_sizes=[128,128],data_manager=dm)\n",
    "bi_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "bi_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "bi_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**BiRNN for LSTM ('lstm') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps, validate for 35 steps\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - 143s 3s/step - loss: 0.7322 - accuracy: 0.7375 - val_loss: 0.2282 - val_accuracy: 0.9243\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - 140s 3s/step - loss: 0.1718 - accuracy: 0.9490 - val_loss: 0.1245 - val_accuracy: 0.9688\n",
      "Epoch 3/20\n",
      "52/52 [==============================] - 145s 3s/step - loss: 0.1033 - accuracy: 0.9688 - val_loss: 0.1037 - val_accuracy: 0.9748\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - 142s 3s/step - loss: 0.0801 - accuracy: 0.9811 - val_loss: 0.1158 - val_accuracy: 0.9679\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - 144s 3s/step - loss: 0.0434 - accuracy: 0.9884 - val_loss: 0.1133 - val_accuracy: 0.9647\n",
      "Epoch 6/20\n",
      "52/52 [==============================] - 142s 3s/step - loss: 0.0274 - accuracy: 0.9933 - val_loss: 0.1433 - val_accuracy: 0.9697\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - 147s 3s/step - loss: 0.0137 - accuracy: 0.9957 - val_loss: 0.9482 - val_accuracy: 0.8550\n",
      "Epoch 8/20\n",
      "52/52 [==============================] - 150s 3s/step - loss: 0.0192 - accuracy: 0.9963 - val_loss: 0.3312 - val_accuracy: 0.9509\n",
      "Epoch 9/20\n",
      "52/52 [==============================] - 156s 3s/step - loss: 0.0063 - accuracy: 0.9988 - val_loss: 0.1871 - val_accuracy: 0.9674\n",
      "Epoch 10/20\n",
      "52/52 [==============================] - 149s 3s/step - loss: 0.0068 - accuracy: 0.9982 - val_loss: 0.2193 - val_accuracy: 0.9661\n",
      "Epoch 11/20\n",
      "52/52 [==============================] - 155s 3s/step - loss: 0.0031 - accuracy: 0.9997 - val_loss: 0.2653 - val_accuracy: 0.9670\n",
      "Epoch 12/20\n",
      "52/52 [==============================] - 148s 3s/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.2019 - val_accuracy: 0.9743\n",
      "Epoch 13/20\n",
      "52/52 [==============================] - 146s 3s/step - loss: 0.0048 - accuracy: 0.9991 - val_loss: 0.2634 - val_accuracy: 0.9683\n",
      "Epoch 14/20\n",
      "52/52 [==============================] - 147s 3s/step - loss: 8.6290e-04 - accuracy: 0.9997 - val_loss: 0.3143 - val_accuracy: 0.9656\n",
      "Epoch 15/20\n",
      "52/52 [==============================] - 148s 3s/step - loss: 1.8521e-05 - accuracy: 1.0000 - val_loss: 0.3146 - val_accuracy: 0.9679\n",
      "Epoch 16/20\n",
      "52/52 [==============================] - 149s 3s/step - loss: 0.0621 - accuracy: 0.9945 - val_loss: 0.2339 - val_accuracy: 0.9720\n",
      "Epoch 17/20\n",
      "52/52 [==============================] - 147s 3s/step - loss: 0.0174 - accuracy: 0.9979 - val_loss: 0.3163 - val_accuracy: 0.9651\n",
      "Epoch 18/20\n",
      "52/52 [==============================] - 158s 3s/step - loss: 5.4162e-06 - accuracy: 1.0000 - val_loss: 0.3084 - val_accuracy: 0.9661\n",
      "Epoch 19/20\n",
      "52/52 [==============================] - 147s 3s/step - loss: 3.8446e-06 - accuracy: 1.0000 - val_loss: 0.3049 - val_accuracy: 0.9679\n",
      "Epoch 20/20\n",
      "52/52 [==============================] - 149s 3s/step - loss: 1.7467e-06 - accuracy: 1.0000 - val_loss: 0.3271 - val_accuracy: 0.9683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d0c0c34248>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_rnn = BiRNN(cell_type=\"lstm\",embed_size=128,state_sizes=[128,128],data_manager=dm)\n",
    "bi_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "bi_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "bi_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same case as before GRU is the clear winner. In this case it got 97.75%, it has been the highest accuracy we've had so far.\n",
    "\n",
    "It's important to notice that bidirectional RNN consumes alot more resources than unidirectional, this is because the information can go forwards or backwards. As it can be show, each epoch in unidirectional takes around 2 mins while bi directional takes around 6 mins. The pictures that appear to have a faster epoch is because google colab was used to speed up the process. At the end, using Bidirectional RNN gave us a 1% improvement, it's not much but when we have high accuracies close to 100%, 1% shows a huge improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RNNs with various types, cells, and fine-tuning embedding matrix for sequence modeling and neural embedding </span> ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**We'll combine the RNN's made before to determine which model gets the highest accuracy.**\n",
    "\n",
    "**Below are the descriptions of the attributes of the class *RNN*:**\n",
    "- `run_mode (self.run_mode)` has three values (scratch, init-only, and init-fine-tune).\n",
    "  - `scratch` means training the embedding matrix from scratch.\n",
    "  - `init-only` means only initialzing the embedding matrix with a pretrained Word2Vect but not further doing fine-tuning that matrix.\n",
    "  - `init-fine-tune` means both initialzing the embedding matrix with a pretrained Word2Vect and further doing fine-tuning that matrix.\n",
    "- `network_type (self.network_type)` has two values (uni-directional and bi-directional) which correspond to either Uni-directional RNN or Bi-directional RNN.\n",
    "- `cell_type (self.cell_type)` has three values (simple-rnn, gru, and lstm) which specify the memory cell used in the network.\n",
    "- `embed_model (self.embed_model)` specifes the pretrained Word2Vect model used.\n",
    "-  `embed_size (self.embed_size)` specifes the embedding size. Note that when run_mode is either init-only' or 'init-fine-tune', this embedding size is extracted from embed_model for dimension compatability.\n",
    "- `state_sizes (self.state_sizes)` indicates the list of the hidden sizes from the second hidden layers of memory cells. For example, $embed\\_size =128$ and $state\\_sizes = [64, 64]$ means that you have three hidden layers in your network with hidden sizes of $128, 64$ and $64$ respectively.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "class RNN:\n",
    "    def __init__(self, run_mode = 'scratch', cell_type= 'gru', network_type = 'uni-directional', embed_model= 'glove-wiki-gigaword-100', \n",
    "                 embed_size= 128, state_sizes = [64, 64], data_manager = None):\n",
    "        self.run_mode = run_mode\n",
    "        self.data_manager = data_manager\n",
    "        self.cell_type = cell_type\n",
    "        self.network_type = network_type\n",
    "        self.state_sizes = state_sizes\n",
    "        self.embed_model = embed_model\n",
    "        self.embed_size = embed_size\n",
    "        if self.run_mode != 'scratch':\n",
    "            self.embed_size = int(self.embed_model.split(\"-\")[-1])\n",
    "        self.data_manager = data_manager\n",
    "        self.vocab_size = dm.vocab_size +1\n",
    "        self.word2idx = dm.word2idx\n",
    "        self.word2vect = None\n",
    "        self.embed_matrix = np.zeros(shape= [self.vocab_size, self.embed_size])\n",
    "    \n",
    "    def build_embedding_matrix(self):\n",
    "        if os.path.exists(\"E.npy\"):  \n",
    "            self.embed_matrix = np.load(\"E.npy\")          \n",
    "        else: \n",
    "            self.word2vect = api.load(self.embed_model)   \n",
    "            for word, idx in self.word2idx.items():\n",
    "                try:\n",
    "                    self.embed_matrix[idx] = self.word2vect.word_vec(word)    \n",
    "                except KeyError: \n",
    "                    pass\n",
    "            np.save(\"E.npy\", self.embed_matrix)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_layer(cell_type= 'gru', network_type= 'uni-directional', state_size= 128, return_sequences= False, activation = 'tanh'):\n",
    "        if network_type== \"uni-directional\":\n",
    "            if cell_type=='gru':\n",
    "                return  tf.keras.layers.GRU(state_size,return_sequences=return_sequences,activation=activation)\n",
    "            elif cell_type== 'lstm':\n",
    "                return tf.keras.layers.LSTM(state_size, return_sequences=return_sequences,activation=activation)\n",
    "            else:\n",
    "                return tf.keras.layers.SimpleRNN(state_size, return_sequences=return_sequences,activation=activation)\n",
    "        if network_type== \"bi-directional\":\n",
    "            if cell_type=='gru':\n",
    "                return  tf.keras.layers.Bidirectional(tf.keras.layers.GRU(state_size, return_sequences=return_sequences,activation=activation))\n",
    "            elif cell_type== 'lstm':\n",
    "                return  tf.keras.layers.Bidirectional((tf.keras.layers.LSTM(state_size, return_sequences=return_sequences,activation=activation)))\n",
    "            else:\n",
    "                return tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(state_size, return_sequences=return_sequences,activation=activation))\n",
    "        \n",
    "    \n",
    "    def build(self):\n",
    "        inputs = tf.keras.layers.Input(shape=[None])\n",
    "        if self.run_mode == \"scratch\":\n",
    "            self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, mask_zero= True, trainable= True)\n",
    "        else: #fine-tuned\n",
    "            self.build_embedding_matrix()\n",
    "            self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embed_size,mask_zero= True, weights= [self.embed_matrix], trainable= True)\n",
    "        num_layers = len(self.state_sizes)\n",
    "        h=tf.keras.layers.Embedding(self.vocab_size, self.embed_size,mask_zero=True, trainable= True)(inputs)\n",
    "        h = self.embedding_layer(inputs)\n",
    "        for i in range(num_layers):\n",
    "            h = self.get_layer(self.cell_type,self.network_type, self.state_sizes[i], return_sequences=True)(h)\n",
    "        h = self.get_layer(self.cell_type,self.network_type, self.state_sizes[i], return_sequences=False)(h)\n",
    "        #h = tf.keras.layers.GRU(256, return_sequences=True)(h)\n",
    "        #h = tf.keras.layers.GRU(128)(h)\n",
    "        h = tf.keras.layers.Dense(dm.num_classes, activation='softmax')(h)\n",
    "        self.model = tf.keras.Model(inputs=inputs, outputs=h)\n",
    "        \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Now we'll compare the 3 models**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps, validate for 35 steps\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - 94s 2s/step - loss: 0.8121 - accuracy: 0.6513 - val_loss: 0.1964 - val_accuracy: 0.9312\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - 88s 2s/step - loss: 0.1294 - accuracy: 0.9587 - val_loss: 0.1501 - val_accuracy: 0.9569\n",
      "Epoch 3/20\n",
      "52/52 [==============================] - 89s 2s/step - loss: 0.0576 - accuracy: 0.9844 - val_loss: 0.0947 - val_accuracy: 0.9683\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - 89s 2s/step - loss: 0.0275 - accuracy: 0.9924 - val_loss: 0.1249 - val_accuracy: 0.9674\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - 90s 2s/step - loss: 0.0200 - accuracy: 0.9942 - val_loss: 0.1824 - val_accuracy: 0.9587\n",
      "Epoch 6/20\n",
      "52/52 [==============================] - 89s 2s/step - loss: 0.0214 - accuracy: 0.9957 - val_loss: 0.1311 - val_accuracy: 0.9647\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - 92s 2s/step - loss: 0.0093 - accuracy: 0.9966 - val_loss: 0.1164 - val_accuracy: 0.9674\n",
      "Epoch 8/20\n",
      "52/52 [==============================] - 90s 2s/step - loss: 0.0044 - accuracy: 0.9985 - val_loss: 0.1261 - val_accuracy: 0.9688\n",
      "Epoch 9/20\n",
      "52/52 [==============================] - 93s 2s/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.1715 - val_accuracy: 0.9679\n",
      "Epoch 10/20\n",
      "52/52 [==============================] - 93s 2s/step - loss: 5.3865e-05 - accuracy: 1.0000 - val_loss: 0.1884 - val_accuracy: 0.9702\n",
      "Epoch 11/20\n",
      "52/52 [==============================] - 92s 2s/step - loss: 8.3560e-06 - accuracy: 1.0000 - val_loss: 0.2155 - val_accuracy: 0.9697\n",
      "Epoch 12/20\n",
      "52/52 [==============================] - 90s 2s/step - loss: 1.2581e-04 - accuracy: 1.0000 - val_loss: 0.2570 - val_accuracy: 0.9720\n",
      "Epoch 13/20\n",
      "52/52 [==============================] - 91s 2s/step - loss: 6.9365e-07 - accuracy: 1.0000 - val_loss: 0.2503 - val_accuracy: 0.9729\n",
      "Epoch 14/20\n",
      "52/52 [==============================] - 92s 2s/step - loss: 3.2126e-07 - accuracy: 1.0000 - val_loss: 0.2551 - val_accuracy: 0.9739\n",
      "Epoch 15/20\n",
      "52/52 [==============================] - 93s 2s/step - loss: 1.4832e-07 - accuracy: 1.0000 - val_loss: 0.2661 - val_accuracy: 0.9739\n",
      "Epoch 16/20\n",
      "52/52 [==============================] - 91s 2s/step - loss: 6.5634e-08 - accuracy: 1.0000 - val_loss: 0.2773 - val_accuracy: 0.9729\n",
      "Epoch 17/20\n",
      "52/52 [==============================] - 93s 2s/step - loss: 3.2566e-08 - accuracy: 1.0000 - val_loss: 0.2849 - val_accuracy: 0.9729\n",
      "Epoch 18/20\n",
      "52/52 [==============================] - 90s 2s/step - loss: 2.2180e-08 - accuracy: 1.0000 - val_loss: 0.2874 - val_accuracy: 0.9729\n",
      "Epoch 19/20\n",
      "52/52 [==============================] - 92s 2s/step - loss: 1.7271e-08 - accuracy: 1.0000 - val_loss: 0.2894 - val_accuracy: 0.9729\n",
      "Epoch 20/20\n",
      "52/52 [==============================] - 91s 2s/step - loss: 1.4548e-08 - accuracy: 1.0000 - val_loss: 0.2913 - val_accuracy: 0.9729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1332cca7508>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn2 = RNN(run_mode=\"scratch\", network_type=\"uni-directional\",cell_type=\"gru\",data_manager=dm,embed_size=128,state_sizes=[128,128])\n",
    "rnn2.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "rnn2.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "rnn2.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps, validate for 35 steps\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - 106s 2s/step - loss: 0.7364 - accuracy: 0.7185 - val_loss: 0.1938 - val_accuracy: 0.9390\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - 97s 2s/step - loss: 0.1503 - accuracy: 0.9487 - val_loss: 0.0903 - val_accuracy: 0.9670\n",
      "Epoch 3/20\n",
      "52/52 [==============================] - 97s 2s/step - loss: 0.0920 - accuracy: 0.9661 - val_loss: 0.0691 - val_accuracy: 0.9748\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - 99s 2s/step - loss: 0.0629 - accuracy: 0.9795 - val_loss: 0.0721 - val_accuracy: 0.9725\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - 99s 2s/step - loss: 0.0501 - accuracy: 0.9832 - val_loss: 0.0596 - val_accuracy: 0.9784\n",
      "Epoch 6/20\n",
      "52/52 [==============================] - 102s 2s/step - loss: 0.0671 - accuracy: 0.9890 - val_loss: 0.1309 - val_accuracy: 0.9642\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - 101s 2s/step - loss: 0.0229 - accuracy: 0.9930 - val_loss: 0.0660 - val_accuracy: 0.9803\n",
      "Epoch 8/20\n",
      "52/52 [==============================] - 100s 2s/step - loss: 0.0196 - accuracy: 0.9930 - val_loss: 0.0694 - val_accuracy: 0.9807\n",
      "Epoch 9/20\n",
      "52/52 [==============================] - 102s 2s/step - loss: 0.0164 - accuracy: 0.9945 - val_loss: 0.0785 - val_accuracy: 0.9789\n",
      "Epoch 10/20\n",
      "52/52 [==============================] - 101s 2s/step - loss: 0.0096 - accuracy: 0.9963 - val_loss: 0.0820 - val_accuracy: 0.9830\n",
      "Epoch 11/20\n",
      "52/52 [==============================] - 104s 2s/step - loss: 0.0097 - accuracy: 0.9963 - val_loss: 0.0733 - val_accuracy: 0.9803\n",
      "Epoch 12/20\n",
      "52/52 [==============================] - 104s 2s/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0815 - val_accuracy: 0.9821\n",
      "Epoch 13/20\n",
      "52/52 [==============================] - 105s 2s/step - loss: 0.0230 - accuracy: 0.9969 - val_loss: 0.1017 - val_accuracy: 0.9775\n",
      "Epoch 14/20\n",
      "52/52 [==============================] - 104s 2s/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0613 - val_accuracy: 0.9872\n",
      "Epoch 15/20\n",
      "52/52 [==============================] - 106s 2s/step - loss: 8.1713e-05 - accuracy: 1.0000 - val_loss: 0.0756 - val_accuracy: 0.9872\n",
      "Epoch 16/20\n",
      "52/52 [==============================] - 105s 2s/step - loss: 0.0084 - accuracy: 0.9991 - val_loss: 0.0874 - val_accuracy: 0.9849\n",
      "Epoch 17/20\n",
      "52/52 [==============================] - 104s 2s/step - loss: 1.4344e-05 - accuracy: 1.0000 - val_loss: 0.0887 - val_accuracy: 0.9844\n",
      "Epoch 18/20\n",
      "52/52 [==============================] - 105s 2s/step - loss: 3.2859e-06 - accuracy: 1.0000 - val_loss: 0.0972 - val_accuracy: 0.9858\n",
      "Epoch 19/20\n",
      "52/52 [==============================] - 105s 2s/step - loss: 8.8315e-07 - accuracy: 1.0000 - val_loss: 0.1155 - val_accuracy: 0.9858\n",
      "Epoch 20/20\n",
      "52/52 [==============================] - 105s 2s/step - loss: 2.1248e-07 - accuracy: 1.0000 - val_loss: 0.1309 - val_accuracy: 0.9858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x133899e22c8>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn2 = RNN(run_mode=\"init-fine-tune\", network_type=\"uni-directional\",cell_type=\"gru\",data_manager=dm,embed_size=128,state_sizes=[128,128])\n",
    "rnn2.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "rnn2.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "rnn2.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 8s 239ms/step - loss: 0.1309 - accuracy: 0.9858\n"
     ]
    }
   ],
   "source": [
    "rnn2.evaluate(dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**WWe'll run the RNN model changing the 5 parameters**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are the codes of the models used, it was run in google colab to speed up the process, images are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn2 = RNN(run_mode=\"init-fine-tune\", network_type=\"uni-directional\",cell_type=\"lstm\",data_manager=dm,embed_size=128,state_sizes=[128,128])\n",
    "rnn2.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "rnn2.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "rnn2.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn3 = RNN(run_mode=\"init-fine-tune\", network_type=\"bi-directional\",cell_type=\"gru\",data_manager=dm,embed_size=128,state_sizes=[128,128])\n",
    "rnn3.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "rnn3.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "rnn3.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn4 = RNN(run_mode=\"init-fine-tune\", network_type=\"bi-directional\",cell_type=\"lstm\",data_manager=dm,embed_size=128,state_sizes=[128,128])\n",
    "rnn4.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "rnn4.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "rnn4.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn5 = RNN(run_mode=\"init-fine-tune\", network_type=\"bi-directional\",cell_type=\"SimpleRNN\",data_manager=dm,embed_size=128,state_sizes=[128,128])\n",
    "rnn5.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "rnn5.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "rnn5.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn6 = RNN(run_mode=\"init-fine-tune\", network_type=\"uni-directional\",cell_type=\"SimpleRNN\",data_manager=dm,embed_size=128,state_sizes=[128,128])\n",
    "rnn6.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "rnn6.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "rnn6.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Report your results here\n",
    "\n",
    "Model 1 (run_mode ='init-fine-tune',...): accuracy =\n",
    "<img src=\"Images/RNN1.png\">\n",
    "<img src=\"Images/RNN2.png\">\n",
    "<img src=\"Images/RNN3.png\">\n",
    "<img src=\"Images/RNN4.png\">\n",
    "<img src=\"Images/RNN5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn3 = RNN(run_mode=\"init-fine-tune\", network_type=\"bi-directional\",cell_type=\"gru\",data_manager=dm,embed_size=128,state_sizes=[128,128])\n",
    "rnn3.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "rnn3.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "rnn3.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model obtaines was a bidirection RNN with GRU and a learning rate of 0.001. This yielded an accuracy of 98.74%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
